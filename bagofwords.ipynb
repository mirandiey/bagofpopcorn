{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5543b70f-f0af-4ade-acba-bda6bbd60ded",
   "metadata": {},
   "source": [
    "#Bag of Words Meets Bag of Popcorn#\n",
    "\n",
    "This notebook will provide a tutorial in using Word2Vectors to get a deeper understanding of sentiment analysis. \n",
    "\n",
    "Some of the libraries we will use are:\n",
    "\n",
    "- Beautiful Soup\n",
    "- nltk\n",
    "- logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876218f-ee14-4774-ac72-785902fee033",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee53bbca-7f40-48e9-bbea-d6c618f4719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#local packages\n",
    "!pip install numpy pandas nltk gensim scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d44679f-1499-4224-895c-e37b6ebe6e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18587121-68eb-493c-a65f-f967aa34f7ad",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ce21ff8-c2c2-4a83-8355-751baa346a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "print(\"Read {} labeled train reviews, {} labeled test reviews, and {} unlabeled reviews\\n\".format(\n",
    "    train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0318eda-951e-48a5-b211-8788a3de613a",
   "metadata": {},
   "source": [
    "Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffe3d317-224e-4196-bf22-af7483270010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n",
      "Parsing sentences from unlabeled set...\n"
     ]
    }
   ],
   "source": [
    "def review_to_sentences(review, remove_stopwords=True):\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    raw_sentences = sent_tokenize(review_text)\n",
    "    sentences = []\n",
    "    stop_words = set(stopwords.words(\"english\")) if remove_stopwords else set()\n",
    "\n",
    "    for raw_sentence in raw_sentences:\n",
    "        sentence_text = re.sub(\"[^a-zA-Z]\", \" \", raw_sentence)\n",
    "        words = word_tokenize(sentence_text.lower())\n",
    "        if remove_stopwords:\n",
    "            words = [w for w in words if w not in stop_words]\n",
    "        if words:\n",
    "            sentences.append(words)\n",
    "    return sentences\n",
    "\n",
    "# 4. Parse all reviews into sentences\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set...\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set...\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73677b-4ca6-4af6-87e1-d923e524000f",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32695c42-be1c-44a7-bf71-6ff74e5f931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model...\")\n",
    "model = Word2Vec(sentences, vector_size=300, window=10, min_count=40, workers=4)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae8c68-82c5-404c-8b50-a127636c9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vector(words, model, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "    \n",
    "    if n_words > 0:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "def get_avg_feature_vectors(reviews, model, num_features):\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    for i, review in enumerate(reviews):\n",
    "        review_feature_vecs[i] = make_feature_vector(review, model, num_features)\n",
    "    return review_feature_vecs\n",
    "\n",
    "\n",
    "print(\"Tokenizing reviews...\")\n",
    "clean_train_reviews = [word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                          BeautifulSoup(review, \"html.parser\").get_text().lower())) \n",
    "                       for review in train[\"review\"]]\n",
    "\n",
    "clean_test_reviews = [word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                         BeautifulSoup(review, \"html.parser\").get_text().lower())) \n",
    "                      for review in test[\"review\"]]\n",
    "\n",
    "\n",
    "print(\"Creating average feature vectors...\")\n",
    "train_data_vecs = get_avg_feature_vectors(clean_train_reviews, model, 300)\n",
    "test_data_vecs = get_avg_feature_vectors(clean_test_reviews, model, 300)\n",
    "\n",
    "print(\"Training the classifier...\")\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(train_data_vecs, train[\"sentiment\"])\n",
    "\n",
    "print(\"Predicting on test set...\")\n",
    "predictions = forest.predict(test_data_vecs)\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": predictions})\n",
    "output.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592653e7-eb3a-4e77-8c45-a5400be6042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(reviews):\n",
    "    clean_reviews = []\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "        words = word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", review_text.lower()))\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        clean_reviews.append(words)\n",
    "    \n",
    "    return clean_reviews\n",
    "\n",
    "clean_train_reviews = tokenize_reviews(train[\"review\"])\n",
    "clean_test_reviews = tokenize_reviews(test[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe937c-20e5-4b29-a2ee-d779eb4ae46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vecs = get_avg_feature_vectors(clean_train_reviews, model, 300)\n",
    "test_data_vecs = get_avg_feature_vectors(clean_test_reviews, model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77cb25-b787-4ce1-9f7e-6ba36e550b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(test_data_vecs)\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": predictions})\n",
    "output.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fa171-c7ed-4e47-b451-abf7d3eccc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
